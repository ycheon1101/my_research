{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 5 / use transform 1/ data set num: 100 / optim: SGD/ batch: 64\n",
    "\n",
    "Epoch [1/5], Loss: 0.6829988956451416\n",
    "Epoch [2/5], Loss: 0.660178005695343\n",
    "Epoch [3/5], Loss: 0.6521103978157043\n",
    "Epoch [4/5], Loss: 0.6584694385528564\n",
    "Epoch [5/5], Loss: 0.6707316040992737\n",
    "checking accuracy on Training set\n",
    "got 57 / 100 with accracy 56.99999999999999\n",
    "checking accuracy on Test set\n",
    "got 8 / 20 with accracy 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 2\n",
    "# transform = transforms.Compose([\n",
    "#    transforms.Resize((224, 224)),\n",
    "#    ToTensor(),\n",
    "# #    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "#     transforms.Normalize(mean, std)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 3\n",
    "# transform = transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std, inplace=False)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    image, label = dataset[i]\n",
    "    print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "    break  \n",
    "\n",
    "print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "    break  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "    # training mode\n",
    "    model.train()\n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 1/ data set num: 100 / optim: SGD/ batch: 64\n",
    "\n",
    "Epoch [1/10], Loss: 0.688021719455719\n",
    "Epoch [2/10], Loss: 0.6919488906860352\n",
    "Epoch [3/10], Loss: 0.6964294910430908\n",
    "Epoch [4/10], Loss: 0.6756751537322998\n",
    "Epoch [5/10], Loss: 0.6902875304222107\n",
    "Epoch [6/10], Loss: 0.6616805195808411\n",
    "Epoch [7/10], Loss: 0.6635900735855103\n",
    "Epoch [8/10], Loss: 0.6734824776649475\n",
    "Epoch [9/10], Loss: 0.6482532620429993\n",
    "Epoch [10/10], Loss: 0.644137978553772\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [10/10], Loss: 0.644137978553772\n",
    "checking accuracy on Training set\n",
    "got 62 / 100 with accracy 62.0\n",
    "checking accuracy on Test set\n",
    "got 10 / 20 with accracy 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "    # training mode\n",
    "    model.train()\n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 1/ data set num: 25000 / optim: SGD/ batch: 64\n",
    "\n",
    "Epoch [1/10], Loss: 0.6884214878082275\n",
    "Epoch [2/10], Loss: 0.6389276385307312\n",
    "Epoch [3/10], Loss: 0.6553725600242615\n",
    "Epoch [4/10], Loss: 0.6433557868003845\n",
    "Epoch [5/10], Loss: 0.6392791271209717\n",
    "Epoch [6/10], Loss: 0.659751296043396\n",
    "Epoch [7/10], Loss: 0.6316360235214233\n",
    "Epoch [8/10], Loss: 0.600490391254425\n",
    "Epoch [9/10], Loss: 0.6885417699813843\n",
    "Epoch [10/10], Loss: 0.6437572240829468\n",
    "checking accuracy on Training set\n",
    "got 13286 / 20000 with accracy 66.43\n",
    "checking accuracy on Test set\n",
    "got 3225 / 5000 with accracy 64.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use randomresizedcrop when calc mean, std/ num_epoch = 10 / use transform 3/ data set num: 100 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.6595055460929871\n",
    "Epoch [2/10], Loss: 0.581857442855835\n",
    "Epoch [3/10], Loss: 0.6393834948539734\n",
    "Epoch [4/10], Loss: 0.8630320429801941\n",
    "Epoch [5/10], Loss: 0.6885052919387817\n",
    "Epoch [6/10], Loss: 0.5494183897972107\n",
    "Epoch [7/10], Loss: 0.5923810601234436\n",
    "Epoch [8/10], Loss: 0.6690357327461243\n",
    "Epoch [9/10], Loss: 0.7105026245117188\n",
    "Epoch [10/10], Loss: 0.6995693445205688\n",
    "checking accuracy on Training set\n",
    "got 54 / 100 with accracy 54.0\n",
    "checking accuracy on Test set\n",
    "got 9 / 20 with accracy 45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "# from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 3\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4875, 0.4498, 0.4112], [0.2132, 0.2091, 0.2080], inplace=False),\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use randomresizedcrop when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 100 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.7946253418922424\n",
    "Epoch [2/10], Loss: 0.6217297911643982\n",
    "Epoch [3/10], Loss: 0.6169763803482056\n",
    "Epoch [4/10], Loss: 0.5657680034637451\n",
    "Epoch [5/10], Loss: 0.514208972454071\n",
    "Epoch [6/10], Loss: 0.6392225623130798\n",
    "Epoch [7/10], Loss: 0.41998255252838135\n",
    "Epoch [8/10], Loss: 0.5450870990753174\n",
    "Epoch [9/10], Loss: 0.41531461477279663\n",
    "Epoch [10/10], Loss: 0.37717658281326294\n",
    "checking accuracy on Training set\n",
    "got 97 / 100 with accracy 97.0\n",
    "checking accuracy on Test set\n",
    "got 10 / 20 with accracy 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 100 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.7497851848602295\n",
    "Epoch [2/10], Loss: 0.6210096478462219\n",
    "Epoch [3/10], Loss: 0.5349128246307373\n",
    "Epoch [4/10], Loss: 0.5577841401100159\n",
    "Epoch [5/10], Loss: 0.7867729067802429\n",
    "Epoch [6/10], Loss: 0.8586550354957581\n",
    "Epoch [7/10], Loss: 0.44590994715690613\n",
    "Epoch [8/10], Loss: 0.4820903241634369\n",
    "Epoch [9/10], Loss: 0.3990113139152527\n",
    "Epoch [10/10], Loss: 0.27303779125213623\n",
    "checking accuracy on Training set\n",
    "got 99 / 100 with accracy 99.0\n",
    "checking accuracy on Test set\n",
    "got 9 / 20 with accracy 45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 100 / optim: SGD/ batch: 32 / add sigmoid/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.6924512982368469\n",
    "Epoch [2/10], Loss: 0.6731902360916138\n",
    "Epoch [3/10], Loss: 0.7045927047729492\n",
    "Epoch [4/10], Loss: 0.6608633399009705\n",
    "Epoch [5/10], Loss: 0.7139399647712708\n",
    "Epoch [6/10], Loss: 0.6770305633544922\n",
    "Epoch [7/10], Loss: 0.6662049293518066\n",
    "Epoch [8/10], Loss: 0.7075626850128174\n",
    "Epoch [9/10], Loss: 0.6222336292266846\n",
    "Epoch [10/10], Loss: 0.6222627758979797\n",
    "checking accuracy on Training set\n",
    "got 76 / 100 with accracy 76.0\n",
    "checking accuracy on Test set\n",
    "got 7 / 20 with accracy 35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from cat_and_dog_dataset.ipynb\n",
      "importing Jupyter notebook from img_normal.ipynb\n",
      "Mean: tensor([0.4883, 0.4551, 0.4170])\n",
      "Standard Deviation: tensor([0.2256, 0.2211, 0.2213])\n",
      "Epoch [1/10], Loss: 0.6924512982368469\n",
      "Epoch [2/10], Loss: 0.6731902360916138\n",
      "Epoch [3/10], Loss: 0.7045927047729492\n",
      "Epoch [4/10], Loss: 0.6608633399009705\n",
      "Epoch [5/10], Loss: 0.7139399647712708\n",
      "Epoch [6/10], Loss: 0.6770305633544922\n",
      "Epoch [7/10], Loss: 0.6662049293518066\n",
      "Epoch [8/10], Loss: 0.7075626850128174\n",
      "Epoch [9/10], Loss: 0.6222336292266846\n",
      "Epoch [10/10], Loss: 0.6222627758979797\n",
      "checking accuracy on Training set\n",
      "got 76 / 100 with accracy 76.0\n",
      "checking accuracy on Test set\n",
      "got 7 / 20 with accracy 35.0\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 25000 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer/ labels.float() -> labels when crossEntropy\n",
    "\n",
    "Epoch [1/10], Loss: 0.7786927819252014 \n",
    "Epoch [2/10], Loss: 0.6254715919494629 \n",
    "Epoch [3/10], Loss: 0.5060574412345886 \n",
    "Epoch [4/10], Loss: 0.5186963677406311 \n",
    "Epoch [5/10], Loss: 0.7118619680404663 \n",
    "Epoch [6/10], Loss: 0.4992092549800873 \n",
    "Epoch [7/10], Loss: 0.5119767785072327 \n",
    "Epoch [8/10], Loss: 0.6405879855155945 \n",
    "Epoch [9/10], Loss: 0.49774616956710815 \n",
    "Epoch [10/10], Loss: 0.5938100218772888 \n",
    "checking accuracy on Training set got 16223 / 20000 with accracy 81.11500000000001 checking accuracy on Test set got 3410 / 5000 with accracy 68.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels)\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ac516125b3ee16d4eaa41f008fade7bad50b808bd1b6d74d2e8ae0015ba9066"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
