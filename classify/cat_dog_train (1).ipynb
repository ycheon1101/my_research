{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 5 / use transform 1/ data set num: 100 / optim: SGD/ batch: 64\n",
    "\n",
    "Epoch [1/5], Loss: 0.6829988956451416\n",
    "Epoch [2/5], Loss: 0.660178005695343\n",
    "Epoch [3/5], Loss: 0.6521103978157043\n",
    "Epoch [4/5], Loss: 0.6584694385528564\n",
    "Epoch [5/5], Loss: 0.6707316040992737\n",
    "checking accuracy on Training set\n",
    "got 57 / 100 with accracy 56.99999999999999\n",
    "checking accuracy on Test set\n",
    "got 8 / 20 with accracy 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 2\n",
    "# transform = transforms.Compose([\n",
    "#    transforms.Resize((224, 224)),\n",
    "#    ToTensor(),\n",
    "# #    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "#     transforms.Normalize(mean, std)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 3\n",
    "# transform = transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std, inplace=False)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    image, label = dataset[i]\n",
    "    print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "    break  \n",
    "\n",
    "print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "    break  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "    # training mode\n",
    "    model.train()\n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 1/ data set num: 100 / optim: SGD/ batch: 64\n",
    "\n",
    "Epoch [1/10], Loss: 0.688021719455719\n",
    "Epoch [2/10], Loss: 0.6919488906860352\n",
    "Epoch [3/10], Loss: 0.6964294910430908\n",
    "Epoch [4/10], Loss: 0.6756751537322998\n",
    "Epoch [5/10], Loss: 0.6902875304222107\n",
    "Epoch [6/10], Loss: 0.6616805195808411\n",
    "Epoch [7/10], Loss: 0.6635900735855103\n",
    "Epoch [8/10], Loss: 0.6734824776649475\n",
    "Epoch [9/10], Loss: 0.6482532620429993\n",
    "Epoch [10/10], Loss: 0.644137978553772\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [10/10], Loss: 0.644137978553772\n",
    "checking accuracy on Training set\n",
    "got 62 / 100 with accracy 62.0\n",
    "checking accuracy on Test set\n",
    "got 10 / 20 with accracy 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "    # training mode\n",
    "    model.train()\n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 1/ data set num: 25000 / optim: SGD/ batch: 64\n",
    "\n",
    "Epoch [1/10], Loss: 0.6884214878082275\n",
    "Epoch [2/10], Loss: 0.6389276385307312\n",
    "Epoch [3/10], Loss: 0.6553725600242615\n",
    "Epoch [4/10], Loss: 0.6433557868003845\n",
    "Epoch [5/10], Loss: 0.6392791271209717\n",
    "Epoch [6/10], Loss: 0.659751296043396\n",
    "Epoch [7/10], Loss: 0.6316360235214233\n",
    "Epoch [8/10], Loss: 0.600490391254425\n",
    "Epoch [9/10], Loss: 0.6885417699813843\n",
    "Epoch [10/10], Loss: 0.6437572240829468\n",
    "checking accuracy on Training set\n",
    "got 13286 / 20000 with accracy 66.43\n",
    "checking accuracy on Test set\n",
    "got 3225 / 5000 with accracy 64.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use randomresizedcrop when calc mean, std/ num_epoch = 10 / use transform 3/ data set num: 100 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.6595055460929871\n",
    "Epoch [2/10], Loss: 0.581857442855835\n",
    "Epoch [3/10], Loss: 0.6393834948539734\n",
    "Epoch [4/10], Loss: 0.8630320429801941\n",
    "Epoch [5/10], Loss: 0.6885052919387817\n",
    "Epoch [6/10], Loss: 0.5494183897972107\n",
    "Epoch [7/10], Loss: 0.5923810601234436\n",
    "Epoch [8/10], Loss: 0.6690357327461243\n",
    "Epoch [9/10], Loss: 0.7105026245117188\n",
    "Epoch [10/10], Loss: 0.6995693445205688\n",
    "checking accuracy on Training set\n",
    "got 54 / 100 with accracy 54.0\n",
    "checking accuracy on Test set\n",
    "got 9 / 20 with accracy 45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "# from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 3\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4875, 0.4498, 0.4112], [0.2132, 0.2091, 0.2080], inplace=False),\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use randomresizedcrop when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 100 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.7946253418922424\n",
    "Epoch [2/10], Loss: 0.6217297911643982\n",
    "Epoch [3/10], Loss: 0.6169763803482056\n",
    "Epoch [4/10], Loss: 0.5657680034637451\n",
    "Epoch [5/10], Loss: 0.514208972454071\n",
    "Epoch [6/10], Loss: 0.6392225623130798\n",
    "Epoch [7/10], Loss: 0.41998255252838135\n",
    "Epoch [8/10], Loss: 0.5450870990753174\n",
    "Epoch [9/10], Loss: 0.41531461477279663\n",
    "Epoch [10/10], Loss: 0.37717658281326294\n",
    "checking accuracy on Training set\n",
    "got 97 / 100 with accracy 97.0\n",
    "checking accuracy on Test set\n",
    "got 10 / 20 with accracy 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 100 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.7497851848602295\n",
    "Epoch [2/10], Loss: 0.6210096478462219\n",
    "Epoch [3/10], Loss: 0.5349128246307373\n",
    "Epoch [4/10], Loss: 0.5577841401100159\n",
    "Epoch [5/10], Loss: 0.7867729067802429\n",
    "Epoch [6/10], Loss: 0.8586550354957581\n",
    "Epoch [7/10], Loss: 0.44590994715690613\n",
    "Epoch [8/10], Loss: 0.4820903241634369\n",
    "Epoch [9/10], Loss: 0.3990113139152527\n",
    "Epoch [10/10], Loss: 0.27303779125213623\n",
    "checking accuracy on Training set\n",
    "got 99 / 100 with accracy 99.0\n",
    "checking accuracy on Test set\n",
    "got 9 / 20 with accracy 45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 100 / optim: SGD/ batch: 32 / add sigmoid/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/10], Loss: 0.6924512982368469\n",
    "Epoch [2/10], Loss: 0.6731902360916138\n",
    "Epoch [3/10], Loss: 0.7045927047729492\n",
    "Epoch [4/10], Loss: 0.6608633399009705\n",
    "Epoch [5/10], Loss: 0.7139399647712708\n",
    "Epoch [6/10], Loss: 0.6770305633544922\n",
    "Epoch [7/10], Loss: 0.6662049293518066\n",
    "Epoch [8/10], Loss: 0.7075626850128174\n",
    "Epoch [9/10], Loss: 0.6222336292266846\n",
    "Epoch [10/10], Loss: 0.6222627758979797\n",
    "checking accuracy on Training set\n",
    "got 76 / 100 with accracy 76.0\n",
    "checking accuracy on Test set\n",
    "got 7 / 20 with accracy 35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 10 / use transform 2/ data set num: 25000 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 30 / use transform + centercrop / data set num: 100 / optim: SGD/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/30], Loss: 0.661065936088562\n",
    "Epoch [2/30], Loss: 0.4749096632003784\n",
    "Epoch [3/30], Loss: 0.5973185896873474\n",
    "Epoch [4/30], Loss: 0.6510248184204102\n",
    "Epoch [5/30], Loss: 0.5774214863777161\n",
    "Epoch [6/30], Loss: 0.5248855352401733\n",
    "Epoch [7/30], Loss: 0.7273577451705933\n",
    "Epoch [8/30], Loss: 0.5403749942779541\n",
    "Epoch [9/30], Loss: 0.5342727899551392\n",
    "Epoch [10/30], Loss: 0.4214780330657959\n",
    "Epoch [11/30], Loss: 0.37489351630210876\n",
    "Epoch [12/30], Loss: 0.3546043336391449\n",
    "Epoch [13/30], Loss: 0.38444843888282776\n",
    "Epoch [14/30], Loss: 0.5043830871582031\n",
    "Epoch [15/30], Loss: 0.3135039210319519\n",
    "Epoch [16/30], Loss: 0.3938744068145752\n",
    "Epoch [17/30], Loss: 0.4075440466403961\n",
    "Epoch [18/30], Loss: 0.315226674079895\n",
    "Epoch [19/30], Loss: 0.23798835277557373\n",
    "Epoch [20/30], Loss: 0.3003026843070984\n",
    "Epoch [21/30], Loss: 0.25553005933761597\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [30/30], Loss: 0.25391849875450134\n",
    "checking accuracy on Training set\n",
    "got 100 / 100 with accracy 100.0\n",
    "checking accuracy on Test set\n",
    "got 9 / 20 with accracy 45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 30 / use transform + centercrop / data set num: 100 / optim: SGD + momentum/ batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/30], Loss: 0.6842379570007324\n",
    "Epoch [2/30], Loss: 1.346143364906311\n",
    "Epoch [3/30], Loss: 0.6354720592498779\n",
    "Epoch [4/30], Loss: 0.42943471670150757\n",
    "Epoch [5/30], Loss: 0.22744254767894745\n",
    "Epoch [6/30], Loss: 0.1614459753036499\n",
    "Epoch [7/30], Loss: 0.164921835064888\n",
    "Epoch [8/30], Loss: 0.5951780676841736\n",
    "Epoch [9/30], Loss: 0.14554078876972198\n",
    "Epoch [10/30], Loss: 0.23797444999217987\n",
    "Epoch [11/30], Loss: 0.15400224924087524\n",
    "Epoch [12/30], Loss: 0.15590843558311462\n",
    "Epoch [13/30], Loss: 0.13699601590633392\n",
    "Epoch [14/30], Loss: 0.13801392912864685\n",
    "Epoch [15/30], Loss: 0.1653336137533188\n",
    "Epoch [16/30], Loss: 0.13161039352416992\n",
    "Epoch [17/30], Loss: 0.16372820734977722\n",
    "Epoch [18/30], Loss: 0.15361419320106506\n",
    "Epoch [19/30], Loss: 0.1433015912771225\n",
    "Epoch [20/30], Loss: 0.13895335793495178\n",
    "Epoch [21/30], Loss: 0.1662384569644928\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [30/30], Loss: 0.4052552580833435\n",
    "checking accuracy on Training set\n",
    "got 93 / 100 with accracy 93.0\n",
    "checking accuracy on Test set\n",
    "got 8 / 20 with accracy 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 20 / use transform + centercrop / data set num: 100 / optim: SGD / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer\n",
    "\n",
    "Epoch [1/20], Loss: 0.7280433177947998\n",
    "Epoch [2/20], Loss: 0.6530508399009705\n",
    "Epoch [3/20], Loss: 0.5301629304885864\n",
    "Epoch [4/20], Loss: 0.5787199139595032\n",
    "Epoch [5/20], Loss: 0.5401037931442261\n",
    "Epoch [6/20], Loss: 0.5569936037063599\n",
    "Epoch [7/20], Loss: 0.49162575602531433\n",
    "Epoch [8/20], Loss: 0.38387924432754517\n",
    "Epoch [9/20], Loss: 0.44622543454170227\n",
    "Epoch [10/20], Loss: 0.26795244216918945\n",
    "Epoch [11/20], Loss: 0.4028141498565674\n",
    "Epoch [12/20], Loss: 0.36601412296295166\n",
    "Epoch [13/20], Loss: 0.30876222252845764\n",
    "Epoch [14/20], Loss: 0.3530395030975342\n",
    "Epoch [15/20], Loss: 0.34304285049438477\n",
    "Epoch [16/20], Loss: 0.4777611792087555\n",
    "Epoch [17/20], Loss: 0.3471497893333435\n",
    "Epoch [18/20], Loss: 0.2962343990802765\n",
    "Epoch [19/20], Loss: 0.28462713956832886\n",
    "Epoch [20/20], Loss: 0.2860521674156189\n",
    "checking accuracy on Training set\n",
    "got 99 / 100 with accracy 99.0\n",
    "checking accuracy on Test set\n",
    "got 8 / 20 with accracy 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 20 / use transform + centercrop / data set num: 100 / optim: SGD / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer + model.train(), model.eval()\n",
    "\n",
    "Epoch [1/20], Train Loss: 0.7650482654571533, Test Loss: 0.758185088634491, Test Accuracy: 55.0%\n",
    "Epoch [2/20], Train Loss: 0.7518641948699951, Test Loss: 0.7249711155891418, Test Accuracy: 65.0%\n",
    "Epoch [3/20], Train Loss: 0.5475682616233826, Test Loss: 0.7474795579910278, Test Accuracy: 55.0%\n",
    "Epoch [4/20], Train Loss: 0.470769464969635, Test Loss: 0.7448733448982239, Test Accuracy: 45.0%\n",
    "Epoch [5/20], Train Loss: 0.552834153175354, Test Loss: 0.7176886200904846, Test Accuracy: 60.000003814697266%\n",
    "Epoch [6/20], Train Loss: 0.663266122341156, Test Loss: 0.7068558931350708, Test Accuracy: 45.0%\n",
    "Epoch [7/20], Train Loss: 0.49595630168914795, Test Loss: 0.7391536831855774, Test Accuracy: 50.0%\n",
    "Epoch [8/20], Train Loss: 0.4366358816623688, Test Loss: 0.7618598341941833, Test Accuracy: 50.0%\n",
    "Epoch [9/20], Train Loss: 0.46581751108169556, Test Loss: 0.7799816131591797, Test Accuracy: 50.0%\n",
    "Epoch [10/20], Train Loss: 0.3558882772922516, Test Loss: 0.7590056657791138, Test Accuracy: 45.0%\n",
    "Epoch [11/20], Train Loss: 0.3713186979293823, Test Loss: 0.7488439083099365, Test Accuracy: 55.0%\n",
    "Epoch [12/20], Train Loss: 0.3107163906097412, Test Loss: 0.7626556158065796, Test Accuracy: 55.0%\n",
    "Epoch [13/20], Train Loss: 0.40933090448379517, Test Loss: 0.7605164647102356, Test Accuracy: 55.0%\n",
    "Epoch [14/20], Train Loss: 0.3455810546875, Test Loss: 0.7621035575866699, Test Accuracy: 50.0%\n",
    "Epoch [15/20], Train Loss: 0.3832550644874573, Test Loss: 0.7801468968391418, Test Accuracy: 50.0%\n",
    "Epoch [16/20], Train Loss: 0.3303754925727844, Test Loss: 0.7671874165534973, Test Accuracy: 50.0%\n",
    "Epoch [17/20], Train Loss: 0.3633565306663513, Test Loss: 0.7718254923820496, Test Accuracy: 50.0%\n",
    "Epoch [18/20], Train Loss: 0.34651342034339905, Test Loss: 0.7692532539367676, Test Accuracy: 50.0%\n",
    "Epoch [19/20], Train Loss: 0.2730775475502014, Test Loss: 0.7790290117263794, Test Accuracy: 50.0%\n",
    "Epoch [20/20], Train Loss: 0.3114137053489685, Test Loss: 0.7907519340515137, Test Accuracy: 50.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()  \n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        labels = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_scores = model(images)\n",
    "        loss = criterion(class_scores, labels)\n",
    "\n",
    "        # backward for calculating gradients and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()  \n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)  \n",
    "            labels = labels.to(device)  \n",
    "\n",
    "            # forward\n",
    "            class_scores = model(images)\n",
    "            loss = criterion(class_scores, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = class_scores.argmax(dim=1)\n",
    "            num_correct += (predictions == labels.argmax(1)).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    accuracy = (num_correct / num_samples) * 100.0\n",
    "\n",
    "    return average_loss, accuracy\n",
    "\n",
    "# training and testing loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}, Test Loss: {test_loss}, Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 30 / use transform + centercrop / data set num: 100 / optim: SGD / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer + model.train(), model.eval() \n",
    "\n",
    "Epoch [1/30], Loss: 0.5516689419746399\n",
    "Epoch [2/30], Loss: 0.6553864479064941\n",
    "Epoch [3/30], Loss: 0.5157697796821594\n",
    "Epoch [4/30], Loss: 0.6209410429000854\n",
    "Epoch [5/30], Loss: 0.6378206014633179\n",
    "Epoch [6/30], Loss: 0.5514135956764221\n",
    "Epoch [7/30], Loss: 0.9291189908981323\n",
    "Epoch [8/30], Loss: 0.49721235036849976\n",
    "Epoch [9/30], Loss: 0.7463662624359131\n",
    "Epoch [10/30], Loss: 0.6215643286705017\n",
    "Epoch [11/30], Loss: 0.6313633322715759     \n",
    "Epoch [12/30], Loss: 0.5122519135475159\n",
    "Epoch [13/30], Loss: 0.4463738203048706\n",
    "Epoch [14/30], Loss: 0.5038759112358093\n",
    "Epoch [15/30], Loss: 0.7185090780258179\n",
    "Epoch [16/30], Loss: 0.45670998096466064\n",
    "Epoch [17/30], Loss: 0.4213845133781433\n",
    "Epoch [18/30], Loss: 0.3648272454738617\n",
    "Epoch [19/30], Loss: 0.44255468249320984\n",
    "Epoch [20/30], Loss: 0.5658482909202576\n",
    "Epoch [21/30], Loss: 0.604422390460968\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [30/30], Loss: 0.2924915850162506\n",
    "checking accuracy on Training set\n",
    "got 99 / 100 with accracy 99.0\n",
    "checking accuracy on Test set\n",
    "got 13 / 20 with accracy 65.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "#learning_rate = 1e-3\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            #nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 30 / use transform + centercrop / data set num: 100 / optim: SGD / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer + model.train(), model.eval() / batch normalization\n",
    "\n",
    "Epoch [1/30], Loss: 1.0136306285858154\n",
    "Epoch [2/30], Loss: 0.5256384611129761\n",
    "Epoch [3/30], Loss: 0.5304974913597107\n",
    "Epoch [4/30], Loss: 0.6144054532051086\n",
    "Epoch [5/30], Loss: 0.656862199306488\n",
    "Epoch [6/30], Loss: 0.8965831398963928\n",
    "Epoch [7/30], Loss: 0.6497758030891418\n",
    "Epoch [8/30], Loss: 0.7635592222213745\n",
    "Epoch [9/30], Loss: 0.534446120262146\n",
    "Epoch [10/30], Loss: 0.5434191226959229\n",
    "Epoch [11/30], Loss: 0.5402575731277466\n",
    "Epoch [12/30], Loss: 0.5119021534919739\n",
    "Epoch [13/30], Loss: 0.45309364795684814\n",
    "Epoch [14/30], Loss: 0.4040907025337219\n",
    "Epoch [15/30], Loss: 0.35334813594818115\n",
    "Epoch [16/30], Loss: 0.25867030024528503\n",
    "Epoch [17/30], Loss: 0.25990626215934753\n",
    "Epoch [18/30], Loss: 0.45160946249961853\n",
    "Epoch [19/30], Loss: 0.34137481451034546\n",
    "Epoch [20/30], Loss: 0.3909694254398346\n",
    "Epoch [21/30], Loss: 0.5323057174682617\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [30/30], Loss: 0.5978360176086426\n",
    "checking accuracy on Training set\n",
    "got 100 / 100 with accracy 100.0\n",
    "checking accuracy on Test set\n",
    "got 10 / 20 with accracy 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.BatchNorm1d(10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.BatchNorm1d(5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 17 / use transform + centercrop / data set num: 100 / optim: SGD / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer + model.train(), model.eval() / batch normalization\n",
    "\n",
    "Epoch [1/17], Loss: 0.9410517811775208\n",
    "Epoch [2/17], Loss: 0.7829709053039551\n",
    "Epoch [3/17], Loss: 0.7021951675415039\n",
    "Epoch [4/17], Loss: 0.7969115972518921\n",
    "Epoch [5/17], Loss: 0.34849706292152405\n",
    "Epoch [6/17], Loss: 0.7657426595687866\n",
    "Epoch [7/17], Loss: 0.836965799331665\n",
    "Epoch [8/17], Loss: 0.44204747676849365\n",
    "Epoch [9/17], Loss: 1.0368456840515137\n",
    "Epoch [10/17], Loss: 0.653814435005188\n",
    "Epoch [11/17], Loss: 0.17304733395576477\n",
    "Epoch [12/17], Loss: 0.8492071032524109\n",
    "Epoch [13/17], Loss: 0.3307250738143921\n",
    "Epoch [14/17], Loss: 0.33910369873046875\n",
    "Epoch [15/17], Loss: 0.28491243720054626\n",
    "Epoch [16/17], Loss: 0.34037286043167114\n",
    "Epoch [17/17], Loss: 0.7725914120674133\n",
    "checking accuracy on Training set\n",
    "got 99 / 100 with accracy 99.0\n",
    "checking accuracy on Test set\n",
    "got 8 / 20 with accracy 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 17\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.BatchNorm1d(10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.BatchNorm1d(5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 30 / use transform + centercrop + colorJitter / data set num: 100 / optim: SGD / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer + model.train(), model.eval() \n",
    "\n",
    "Epoch [1/30], Loss: 0.7230271100997925\n",
    "Epoch [2/30], Loss: 0.7384036779403687\n",
    "Epoch [3/30], Loss: 0.7491973042488098\n",
    "Epoch [4/30], Loss: 0.8197749853134155\n",
    "Epoch [5/30], Loss: 0.6364127993583679\n",
    "Epoch [6/30], Loss: 0.6486576199531555\n",
    "Epoch [7/30], Loss: 0.6163468360900879\n",
    "Epoch [8/30], Loss: 0.8640875816345215\n",
    "Epoch [9/30], Loss: 0.6495388150215149\n",
    "Epoch [10/30], Loss: 0.6619004011154175\n",
    "Epoch [11/30], Loss: 0.642125129699707\n",
    "Epoch [12/30], Loss: 0.5758170485496521\n",
    "Epoch [13/30], Loss: 0.6188790798187256\n",
    "Epoch [14/30], Loss: 0.75055992603302\n",
    "Epoch [15/30], Loss: 0.5563116669654846\n",
    "Epoch [16/30], Loss: 0.6785495281219482\n",
    "Epoch [17/30], Loss: 0.6502804756164551\n",
    "Epoch [18/30], Loss: 0.6345024704933167\n",
    "Epoch [19/30], Loss: 0.5741427540779114\n",
    "Epoch [20/30], Loss: 0.5574901700019836\n",
    "Epoch [21/30], Loss: 0.5354763865470886\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [30/30], Loss: 0.6613378524780273\n",
    "checking accuracy on Training set\n",
    "got 92 / 100 with accracy 92.0\n",
    "checking accuracy on Test set\n",
    "got 12 / 20 with accracy 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resize when calc mean, std/ num_epoch = 30 / use transform + centercrop / data set num: 100 / optim: SGD + momentum / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer + model.train(), model.eval() \n",
    "\n",
    "Epoch [1/30], Loss: 0.681688129901886\n",
    "Epoch [2/30], Loss: 0.5502901673316956\n",
    "Epoch [3/30], Loss: 0.5514283180236816\n",
    "Epoch [4/30], Loss: 0.6526256203651428\n",
    "Epoch [5/30], Loss: 0.3460688591003418\n",
    "Epoch [6/30], Loss: 0.37935906648635864\n",
    "Epoch [7/30], Loss: 0.5031934976577759\n",
    "Epoch [8/30], Loss: 0.4837206304073334\n",
    "Epoch [9/30], Loss: 0.45266300439834595\n",
    "Epoch [10/30], Loss: 0.19656500220298767\n",
    "Epoch [11/30], Loss: 0.176497682929039\n",
    "Epoch [12/30], Loss: 0.1445126086473465\n",
    "Epoch [13/30], Loss: 0.22931212186813354\n",
    "Epoch [14/30], Loss: 0.13587144017219543\n",
    "Epoch [15/30], Loss: 0.16981680691242218\n",
    "Epoch [16/30], Loss: 0.15087677538394928\n",
    "Epoch [17/30], Loss: 0.13050831854343414\n",
    "Epoch [18/30], Loss: 0.1320030689239502\n",
    "Epoch [19/30], Loss: 0.3189740478992462\n",
    "Epoch [20/30], Loss: 0.12889617681503296\n",
    "Epoch [21/30], Loss: 0.1521141231060028\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [30/30], Loss: 0.13535752892494202\n",
    "checking accuracy on Training set\n",
    "got 96 / 100 with accracy 96.0\n",
    "checking accuracy on Test set\n",
    "got 10 / 20 with accracy 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #drop out\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(10000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(5000, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        # logit = class score tensor\n",
    "        # print('checking Forward_NeuralNetwork')\n",
    "        return logits\n",
    "\n",
    "# model\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (CNN) Use resize when calc mean, std/ num_epoch = 30 / use transform + centercrop / data set num: 100 / optim: SGD / batch: 32 / add tanh/ dropout location: before input layer, between linear, before output layer + model.train(), model.eval() \n",
    "\n",
    "Epoch [1/30], Loss: 0.6998062133789062\n",
    "Epoch [2/30], Loss: 0.6833628416061401\n",
    "Epoch [3/30], Loss: 0.6833722591400146\n",
    "Epoch [4/30], Loss: 0.7176940441131592\n",
    "Epoch [5/30], Loss: 0.6986485123634338\n",
    "Epoch [6/30], Loss: 0.6862339973449707\n",
    "Epoch [7/30], Loss: 0.6943838596343994\n",
    "Epoch [8/30], Loss: 0.6894701719284058\n",
    "Epoch [9/30], Loss: 0.6983991861343384\n",
    "Epoch [10/30], Loss: 0.7059083580970764\n",
    "Epoch [11/30], Loss: 0.6926674842834473\n",
    "Epoch [12/30], Loss: 0.6815227270126343\n",
    "Epoch [13/30], Loss: 0.6734297871589661\n",
    "Epoch [14/30], Loss: 0.6712950468063354\n",
    "Epoch [15/30], Loss: 0.683769941329956\n",
    "Epoch [16/30], Loss: 0.693181037902832\n",
    "Epoch [17/30], Loss: 0.6790727972984314\n",
    "Epoch [18/30], Loss: 0.6740067005157471\n",
    "Epoch [19/30], Loss: 0.6791797876358032\n",
    "Epoch [20/30], Loss: 0.6501713991165161\n",
    "show more (open the raw output data in a text editor) ...\n",
    "\n",
    "Epoch [30/30], Loss: 0.6508783102035522\n",
    "checking accuracy on Training set\n",
    "got 78 / 100 with accracy 78.0\n",
    "checking accuracy on Test set\n",
    "got 10 / 20 with accracy 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from cat_and_dog_dataset.ipynb\n",
      "Dataset size: 120\n",
      "importing Jupyter notebook from img_normal.ipynb\n",
      "Mean: tensor([0.4883, 0.4551, 0.4170])\n",
      "Standard Deviation: tensor([0.2256, 0.2211, 0.2213])\n",
      "Epoch [1/30], Loss: 0.6998062133789062\n",
      "Epoch [2/30], Loss: 0.6833628416061401\n",
      "Epoch [3/30], Loss: 0.6833722591400146\n",
      "Epoch [4/30], Loss: 0.7176940441131592\n",
      "Epoch [5/30], Loss: 0.6986485123634338\n",
      "Epoch [6/30], Loss: 0.6862339973449707\n",
      "Epoch [7/30], Loss: 0.6943838596343994\n",
      "Epoch [8/30], Loss: 0.6894701719284058\n",
      "Epoch [9/30], Loss: 0.6983991861343384\n",
      "Epoch [10/30], Loss: 0.7059083580970764\n",
      "Epoch [11/30], Loss: 0.6926674842834473\n",
      "Epoch [12/30], Loss: 0.6815227270126343\n",
      "Epoch [13/30], Loss: 0.6734297871589661\n",
      "Epoch [14/30], Loss: 0.6712950468063354\n",
      "Epoch [15/30], Loss: 0.683769941329956\n",
      "Epoch [16/30], Loss: 0.693181037902832\n",
      "Epoch [17/30], Loss: 0.6790727972984314\n",
      "Epoch [18/30], Loss: 0.6740067005157471\n",
      "Epoch [19/30], Loss: 0.6791797876358032\n",
      "Epoch [20/30], Loss: 0.6501713991165161\n",
      "Epoch [21/30], Loss: 0.6671341061592102\n",
      "Epoch [22/30], Loss: 0.6685290336608887\n",
      "Epoch [23/30], Loss: 0.6679968237876892\n",
      "Epoch [24/30], Loss: 0.6743802428245544\n",
      "Epoch [25/30], Loss: 0.6902110576629639\n",
      "Epoch [26/30], Loss: 0.6849168539047241\n",
      "Epoch [27/30], Loss: 0.6751747727394104\n",
      "Epoch [28/30], Loss: 0.6360804438591003\n",
      "Epoch [29/30], Loss: 0.7061045169830322\n",
      "Epoch [30/30], Loss: 0.6508783102035522\n",
      "checking accuracy on Training set\n",
      "got 78 / 100 with accracy 78.0\n",
      "checking accuracy on Test set\n",
      "got 10 / 20 with accracy 50.0\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from cat_and_dog_dataset import CatAndDog\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from img_normal import mean, std\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparam\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "# transform 2\n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.CenterCrop(224),\n",
    "   ToTensor(),\n",
    "#    transforms.Normalize([0.4883, 0.4551, 0.4170], [0.2256, 0.2211, 0.2213], inplace=False)\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load data\n",
    "\n",
    "dataset = CatAndDog('./cat_dog_100.csv', \n",
    "                    './cat_dog/image/', \n",
    "                    # transform = ToTensor(), \n",
    "                    # target_transform=Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "                    )\n",
    "dataset.transform = transform\n",
    "# dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "dataset.target_transform = Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, y.clone().detach(), value=1))\n",
    "\n",
    "\n",
    "# train set = 80%, test_set = 20%\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [100, 20]) \n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# print(\"Checking dataset...\")\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     image, label = dataset[i]\n",
    "#     print(f\"Image {i} shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "# print(\"\\nChecking train loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "# print(\"\\nChecking test loader...\")\n",
    "\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "#     print(f\"Batch images shape: {images.shape}, Batch labels shape: {labels.shape}\")\n",
    "#     break  \n",
    "\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         #drop out\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(3 * 224 * 224, 10000),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "\n",
    "#             nn.Linear(10000, 5000),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            \n",
    "#             nn.Linear(5000, 2),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.dropout(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "\n",
    "#         # logit = class score tensor\n",
    "#         # print('checking Forward_NeuralNetwork')\n",
    "#         return logits\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_output, n_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers (224 * 224 -> 112 * 112 -> 56 * 56 -> 28 * 28)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 1000)\n",
    "        self.fc2 = nn.Linear(1000, n_output)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# model\n",
    "model = CNN(n_output=num_classes, n_hidden=1000).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# calc loss \n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Update the weight\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# training model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # update and store into device\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  \n",
    "        label = labels.to(device)  \n",
    "\n",
    "        # forward\n",
    "        class_score = model(images)\n",
    "        loss = criterion(class_score, labels.float())\n",
    "\n",
    "        # bakcward for calculating gradient and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # test\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# check accuracy\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # forward to check accuracy\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            score = model(x)\n",
    "\n",
    "            prediction = score.argmax(1)\n",
    "            num_correct += (prediction == y.argmax(1)).sum()\n",
    "            num_samples += prediction.size(0)\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with accracy {float(num_correct)/float(num_samples) * 100}')\n",
    "\n",
    "            \n",
    "\n",
    "# test\n",
    "\n",
    "print('checking accuracy on Training set')\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print('checking accuracy on Test set')\n",
    "check_accuracy(test_loader, model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ac516125b3ee16d4eaa41f008fade7bad50b808bd1b6d74d2e8ae0015ba9066"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
